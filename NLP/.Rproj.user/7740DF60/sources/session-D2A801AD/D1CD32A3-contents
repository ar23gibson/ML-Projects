################################################################################
#
#   Title: Natural language processing Kaggle example
#   Author: A. Gibson
#   Date: 15/05/2023
#   Version: 1.0
#   GitRepo:
#   Description:
#
#   url:https://www.kaggle.com/code/rtatman/nlp-in-r-topic-modelling/notebook
#
#   To build an 
#       Unsupervised topic model using Latent Dirichlet Allocation (LDA)
#       Explore the impact of text pre-processing, including removing stop words
#       and stemming
#       Built a supervised topic model using term frequencyâ€“inverse document 
#       frequency (TF-IDF)
#
#       Topic modeling: The NLP task of identifying automatically identifying 
#       major themes in a text, usually by identifying informative words.
#
#       LDA outputs:
#       An estimate of how much each topic contributes to each document
#       An estimate of how much each word contributes to each topic
#
#       Data from:
#       M. Ott, C. Cardie, and J.T. Hancock. 2013. Negative Deceptive Opinion 
#       Spam. In Proceedings of the 2013 Conference of the North American 
#       Chapter of the Association for Computational Linguistics: Human Language
#       Technologies.
#
################################################################################


### Initialise libraries -------------------------------------------------------

library(tidyverse) # general utility & workflow functions
library(tidytext) # tidy implimentation of NLP methods
library(topicmodels) # for LDA topic modelling 
library(tm) # general text mining functions, making document term matrices
library(SnowballC) # for stemming


### Read in data ---------------------------------------------------------------

reviews <- read.csv("C:/Users/andrew.gibson/Documents/GitHub/ML-Projects/NLP/data/deceptive-opinion.csv")
# how to get this to split over two lines?


### Function that reads column; return plot of most informative words for given 
### number of topics ----------------------------------------------------------- 

top_terms_by_topic_LDA <- function(input_text, # should be a column
                               plot = T, # return plot
                               number_of_topics = 4 # default
                               )
{
# create corpus (collection of text sources) and document term matrix
    Corpus <- Corpus(VectorSource(input_text))
    DTM <- DocumentTermMatrix(Corpus)
    
    # remove any empty rows (will cause LDA to crash)
    unique_indexes <- unique(DTM$i) # gives index of each unique value
    DTM <- DTM[unique_indexes,] # get subset of only those indexes
    
    #perform LDA & output words in tidy format
    lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
    topics <- tidy(lda, matrix = "beta")
    
    # retrieve top ten terms for each topic
    top_terms <- topics %>% # take topics data frame
      group_by(topic) %>% # treat each topic as a seperate group
      top_n(10, beta) %>% # get the top 10 most informative words
      ungroup() %>% # ungroup
      arrange(topic, -beta) # arrange words in decending informativeness 
    
    # if the user asks for a plot (TRUE by default)
    if(plot == T){
      # plot the top ten terms for each topic in order
      top_terms %>% # take the top terms
        mutate(term = reorder(term, beta)) %>% # sort terms by beta value 
        ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
        geom_col(show.legend = FALSE) + # as a bar plot
        facet_wrap(~ topic, scales = "free") + # which each topic in a separate plot
        labs(x = NULL, y = "Beta") + # no x label, change y label 
        coord_flip() # turn bars sideways
    }else{ 
      # if the user does not request a plot
      # return a list of sorted terms instead
      return(top_terms)
    }
}

# plot top ten terms in the hotel reviews by topic; wasn't very informative, clean to remove 
# unuseful words
 # top_terms_by_topic_LDA(reviews$text, number_of_topics = 2)

# create DTM to clean
reviewsCorpus <- Corpus(VectorSource(reviews$text))
reviewsDTM <- DocumentTermMatrix(reviewsCorpus)

# create tidy object
reviewsDTM_tidy <- tidy(reviewsDTM)

# create vector of words that aren't useful
custom_stop_words <- tibble(word = c("hotel", "room"))

# remove stopwords
reviewsDTM_tidy_cleaned <- reviewsDTM_tidy %>%
  anti_join(stop_words, by = c("term" = "word")) %>%
  anti_join(custom_stop_words, by = c("term" = "word")) # remove english 
# and custom stop words

# reconstruct cleaned doc so each word shows up the correct number of times
cleaned_documents <- reviewsDTM_tidy_cleaned %>% 
    group_by(document) %>%
    mutate(terms = toString(rep(term,count))) %>%
    select(document, terms) %>%
    unique()

# check doc
head(cleaned_documents)

# plot top ten terms in the hotel reviews by topic; try again with cleaned doc 
#top_terms_by_topic_LDA(cleaned_documents$terms, number_of_topics = 2)
# doc still contains words that are roots of others - remove via stemming

# stem the words (e.g. convert each word to its stem, where applicable)
reviewsDTM_tidy_cleaned <- reviewsDTM_tidy_cleaned %>% 
  mutate(stem = wordStem(term))

# reconstruct our documents
cleaned_documents <- reviewsDTM_tidy_cleaned %>%
  group_by(document) %>% 
  mutate(terms = toString(rep(stem, count))) %>%
  select(document, terms) %>%
  unique()

# now let's look at the new most informative terms
top_terms_by_topic_LDA(cleaned_documents$terms, number_of_topics = 2)
# weird spelling of words: stai - technicaly the root. 
# in general unsupervised is good for exploratory analyis, best use supervised











